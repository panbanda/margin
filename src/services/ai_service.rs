//! AI service for intelligent email features.
//!
//! The [`AiService`] orchestrates AI provider interactions for:
//! - Thread summarization
//! - Draft reply generation
//! - Semantic search
//! - Email categorization
//! - Sender analysis

use std::collections::HashMap;
use std::sync::Arc;

use anyhow::Result;
use serde::{Deserialize, Serialize};
use tokio::sync::RwLock;

use crate::domain::{AccountId, Email, EmailId, SenderAnalysis, Thread};

/// LLM provider trait for abstracting over different AI backends.
///
/// Implementations include OpenAI, Anthropic, Ollama, and Candle (local).
#[async_trait::async_trait]
pub trait LlmProvider: Send + Sync {
    /// Returns the provider name (e.g., "openai", "anthropic").
    fn name(&self) -> &str;

    /// Performs a completion request.
    async fn complete(&self, request: &CompletionRequest) -> Result<CompletionResponse>;

    /// Returns the maximum context length in tokens.
    fn max_context_length(&self) -> usize;
}

/// Embedding engine trait for semantic search.
#[async_trait::async_trait]
pub trait EmbeddingEngine: Send + Sync {
    /// Generates an embedding vector for the given text.
    async fn embed(&self, text: &str) -> Result<Vec<f32>>;

    /// Searches for similar emails by embedding.
    async fn search(&self, query_embedding: &[f32], limit: usize) -> Result<Vec<(EmailId, f32)>>;

    /// Indexes an email for future search.
    async fn index_email(&self, email: &Email) -> Result<()>;
}

/// A request for LLM completion.
#[derive(Debug, Clone)]
pub struct CompletionRequest {
    /// System prompt to set context.
    pub system_prompt: Option<String>,
    /// Conversation messages.
    pub messages: Vec<ChatMessage>,
    /// Temperature for response randomness (0.0-1.0).
    pub temperature: f32,
    /// Maximum tokens to generate.
    pub max_tokens: Option<usize>,
}

/// A message in a chat completion.
#[derive(Debug, Clone)]
pub struct ChatMessage {
    /// Role of the message author.
    pub role: ChatRole,
    /// Message content.
    pub content: String,
}

/// Role in a chat conversation.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum ChatRole {
    /// System message for context.
    System,
    /// User message.
    User,
    /// Assistant (AI) response.
    Assistant,
}

/// Response from an LLM completion.
#[derive(Debug, Clone)]
pub struct CompletionResponse {
    /// Generated text.
    pub text: String,
    /// Token usage statistics.
    pub tokens_used: TokenUsage,
    /// Why generation stopped.
    pub finish_reason: FinishReason,
}

/// Token usage statistics for a completion.
#[derive(Debug, Clone, Default)]
pub struct TokenUsage {
    /// Tokens in the prompt.
    pub prompt_tokens: usize,
    /// Tokens in the response.
    pub completion_tokens: usize,
    /// Total tokens used.
    pub total_tokens: usize,
}

/// Reason why generation finished.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum FinishReason {
    /// Natural completion.
    Stop,
    /// Hit max token limit.
    Length,
    /// Content filter triggered.
    ContentFilter,
}

/// Summary of an email thread.
///
/// Generated by AI to provide a quick overview of a conversation.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Summary {
    /// Concise summary text.
    pub text: String,
    /// Key points extracted from the thread.
    pub key_points: Vec<String>,
    /// Action items or tasks identified.
    pub action_items: Vec<String>,
}

impl Summary {
    /// Parses a summary from LLM output.
    ///
    /// Expects structured output with sections for summary, key points, and action items.
    pub fn parse(text: &str) -> Self {
        // Simple parsing - in production this would use structured output
        let mut summary_text = String::new();
        let mut key_points = Vec::new();
        let mut action_items = Vec::new();

        let mut current_section = "summary";

        for line in text.lines() {
            let line = line.trim();
            if line.is_empty() {
                continue;
            }

            if line.to_lowercase().starts_with("key points:") {
                current_section = "key_points";
                continue;
            } else if line.to_lowercase().starts_with("action items:") {
                current_section = "action_items";
                continue;
            }

            match current_section {
                "summary" => {
                    if !summary_text.is_empty() {
                        summary_text.push(' ');
                    }
                    summary_text.push_str(line);
                }
                "key_points" => {
                    if let Some(point) = line.strip_prefix("- ").or_else(|| line.strip_prefix("* "))
                    {
                        key_points.push(point.to_string());
                    } else if !line.is_empty() {
                        key_points.push(line.to_string());
                    }
                }
                "action_items" => {
                    if let Some(item) = line.strip_prefix("- ").or_else(|| line.strip_prefix("* "))
                    {
                        action_items.push(item.to_string());
                    } else if !line.is_empty() {
                        action_items.push(line.to_string());
                    }
                }
                _ => {}
            }
        }

        Self {
            text: summary_text,
            key_points,
            action_items,
        }
    }
}

/// A suggested draft reply generated by AI.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DraftSuggestion {
    /// Suggested reply content.
    pub content: String,
    /// Confidence score (0.0-1.0) in the suggestion quality.
    pub confidence: f32,
}

/// A search result from semantic search.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SearchResult {
    /// ID of the matching email.
    pub email_id: EmailId,
    /// ID of the thread containing the email.
    pub thread_id: crate::domain::ThreadId,
    /// Subject of the email.
    pub subject: Option<String>,
    /// Preview snippet.
    pub snippet: String,
    /// Relevance score (0.0-1.0).
    pub relevance: f32,
    /// Highlighted matching text.
    pub highlights: Vec<String>,
}

/// Category for email classification.
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum Category {
    /// Primary/important emails.
    Primary,
    /// Social network notifications.
    Social,
    /// Promotional emails.
    Promotions,
    /// Software updates and notifications.
    Updates,
    /// Forum and mailing list messages.
    Forums,
    /// Financial/billing related.
    Finance,
    /// Travel and booking confirmations.
    Travel,
    /// Shopping and order confirmations.
    Shopping,
    /// Newsletters and subscriptions.
    Newsletters,
    /// Work/professional emails.
    Work,
    /// Personal emails.
    Personal,
}

impl Category {
    /// Returns a human-readable name for this category.
    pub fn display_name(&self) -> &'static str {
        match self {
            Category::Primary => "Primary",
            Category::Social => "Social",
            Category::Promotions => "Promotions",
            Category::Updates => "Updates",
            Category::Forums => "Forums",
            Category::Finance => "Finance",
            Category::Travel => "Travel",
            Category::Shopping => "Shopping",
            Category::Newsletters => "Newsletters",
            Category::Work => "Work",
            Category::Personal => "Personal",
        }
    }

    /// Returns all category variants.
    pub fn all() -> &'static [Category] {
        &[
            Category::Primary,
            Category::Social,
            Category::Promotions,
            Category::Updates,
            Category::Forums,
            Category::Finance,
            Category::Travel,
            Category::Shopping,
            Category::Newsletters,
            Category::Work,
            Category::Personal,
        ]
    }
}

/// Settings for AI functionality.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AiSettings {
    /// Whether AI features are enabled.
    pub enabled: bool,
    /// Default provider to use.
    pub default_provider: String,
    /// Settings for summarization.
    pub summary_settings: SummarySettings,
    /// Settings for draft generation.
    pub compose_settings: ComposeSettings,
    /// Settings for semantic search.
    pub search_settings: SearchSettings,
}

impl Default for AiSettings {
    fn default() -> Self {
        Self {
            enabled: true,
            default_provider: "anthropic".to_string(),
            summary_settings: SummarySettings::default(),
            compose_settings: ComposeSettings::default(),
            search_settings: SearchSettings::default(),
        }
    }
}

/// Settings for thread summarization.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SummarySettings {
    /// Whether automatic summarization is enabled.
    pub enabled: bool,
    /// Provider override for summaries.
    pub provider: Option<String>,
    /// System prompt for summarization.
    pub system_prompt: String,
    /// Maximum summary length in characters.
    pub max_length: usize,
}

impl Default for SummarySettings {
    fn default() -> Self {
        Self {
            enabled: true,
            provider: None,
            system_prompt:
                "Summarize this email thread concisely. Include key points and any action items."
                    .to_string(),
            max_length: 500,
        }
    }
}

/// Settings for draft composition.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ComposeSettings {
    /// Whether AI drafts are enabled.
    pub enabled: bool,
    /// Provider override for drafts.
    pub provider: Option<String>,
    /// System prompt for draft generation.
    pub system_prompt: String,
    /// Writing tone preference.
    pub tone: Tone,
    /// Whether to learn from sent emails.
    pub learn_from_sent: bool,
}

impl Default for ComposeSettings {
    fn default() -> Self {
        Self {
            enabled: true,
            provider: None,
            system_prompt: "Draft a reply matching the user's communication style.".to_string(),
            tone: Tone::Casual,
            learn_from_sent: true,
        }
    }
}

/// Writing tone for draft generation.
#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
pub enum Tone {
    /// Formal, professional tone.
    Formal,
    /// Friendly, conversational tone.
    Casual,
    /// Short, to-the-point responses.
    Brief,
    /// Comprehensive, thorough responses.
    Detailed,
    /// User-defined tone with custom prompt.
    Custom(String),
}

/// Settings for semantic search.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SearchSettings {
    /// Whether semantic search is enabled.
    pub enabled: bool,
    /// Model to use for embeddings.
    pub embedding_model: String,
    /// Maximum results to return.
    pub max_results: usize,
    /// Minimum relevance threshold (0.0-1.0).
    pub min_relevance: f32,
}

impl Default for SearchSettings {
    fn default() -> Self {
        Self {
            enabled: true,
            embedding_model: "all-MiniLM-L6-v2".to_string(),
            max_results: 50,
            min_relevance: 0.5,
        }
    }
}

/// AI service for intelligent email features.
///
/// Orchestrates AI providers for summarization, draft generation,
/// semantic search, categorization, and sender analysis.
///
/// # Thread Safety
///
/// AiService uses `Arc` and `RwLock` internally for thread-safe access
/// to providers. All async methods can be called concurrently.
///
/// # Example
///
/// ```ignore
/// let service = AiService::new(settings);
/// service.register_provider("anthropic", provider).await;
///
/// let summary = service.summarize_thread(&thread).await?;
/// let draft = service.draft_reply(&thread, Some("decline politely")).await?;
/// ```
pub struct AiService {
    /// Registered LLM providers by name.
    providers: RwLock<HashMap<String, Arc<dyn LlmProvider>>>,
    /// Embedding engine for semantic search.
    embedding_engine: RwLock<Option<Arc<dyn EmbeddingEngine>>>,
    /// AI settings.
    settings: RwLock<AiSettings>,
}

impl AiService {
    /// Creates a new AiService with the given settings.
    pub fn new(settings: AiSettings) -> Self {
        Self {
            providers: RwLock::new(HashMap::new()),
            embedding_engine: RwLock::new(None),
            settings: RwLock::new(settings),
        }
    }

    /// Registers an LLM provider.
    pub async fn register_provider(&self, name: impl Into<String>, provider: Arc<dyn LlmProvider>) {
        let mut providers = self.providers.write().await;
        providers.insert(name.into(), provider);
    }

    /// Sets the embedding engine for semantic search.
    pub async fn set_embedding_engine(&self, engine: Arc<dyn EmbeddingEngine>) {
        let mut embedding = self.embedding_engine.write().await;
        *embedding = Some(engine);
    }

    /// Updates the AI settings.
    pub async fn update_settings(&self, settings: AiSettings) {
        let mut current = self.settings.write().await;
        *current = settings;
    }

    /// Returns the current AI settings.
    pub async fn settings(&self) -> AiSettings {
        self.settings.read().await.clone()
    }

    /// Gets a provider by name, falling back to the default.
    async fn get_provider(&self, name: Option<&str>) -> Result<Arc<dyn LlmProvider>> {
        let settings = self.settings.read().await;
        let provider_name = name.unwrap_or(&settings.default_provider);

        let providers = self.providers.read().await;
        providers
            .get(provider_name)
            .cloned()
            .ok_or_else(|| anyhow::anyhow!("Provider not found: {}", provider_name))
    }

    /// Summarizes an email thread.
    ///
    /// Uses AI to generate a concise summary with key points and action items.
    ///
    /// # Arguments
    ///
    /// * `thread` - The thread to summarize
    ///
    /// # Returns
    ///
    /// A summary containing text, key points, and action items.
    pub async fn summarize_thread(&self, thread: &Thread) -> Result<Summary> {
        let settings = self.settings.read().await;
        if !settings.enabled || !settings.summary_settings.enabled {
            anyhow::bail!("AI summarization is disabled");
        }

        let provider = self
            .get_provider(settings.summary_settings.provider.as_deref())
            .await?;

        // Build the thread content for summarization
        let thread_content = self.format_thread_for_summary(thread);

        let request = CompletionRequest {
            system_prompt: Some(settings.summary_settings.system_prompt.clone()),
            messages: vec![ChatMessage {
                role: ChatRole::User,
                content: thread_content,
            }],
            temperature: 0.3,
            max_tokens: Some(settings.summary_settings.max_length),
        };

        let response = provider.complete(&request).await?;
        Ok(Summary::parse(&response.text))
    }

    /// Generates a draft reply for a thread.
    ///
    /// Uses AI to draft a contextually appropriate reply based on
    /// the thread history and optional instructions.
    ///
    /// # Arguments
    ///
    /// * `thread` - The thread to reply to
    /// * `instructions` - Optional specific instructions (e.g., "decline politely")
    ///
    /// # Returns
    ///
    /// A draft suggestion with content and confidence score.
    pub async fn draft_reply(
        &self,
        thread: &Thread,
        instructions: Option<&str>,
    ) -> Result<DraftSuggestion> {
        let settings = self.settings.read().await;
        if !settings.enabled || !settings.compose_settings.enabled {
            anyhow::bail!("AI drafting is disabled");
        }

        let provider = self
            .get_provider(settings.compose_settings.provider.as_deref())
            .await?;

        let thread_content = self.format_thread_for_reply(thread);
        let tone_instruction = match &settings.compose_settings.tone {
            Tone::Formal => "Use a formal, professional tone.",
            Tone::Casual => "Use a friendly, casual tone.",
            Tone::Brief => "Keep the response brief and to the point.",
            Tone::Detailed => "Provide a comprehensive, detailed response.",
            Tone::Custom(prompt) => prompt,
        };

        let mut user_content = format!("{}\n\nDraft a reply. {}", thread_content, tone_instruction);

        if let Some(inst) = instructions {
            user_content.push_str(&format!("\n\nAdditional instructions: {}", inst));
        }

        let request = CompletionRequest {
            system_prompt: Some(settings.compose_settings.system_prompt.clone()),
            messages: vec![ChatMessage {
                role: ChatRole::User,
                content: user_content,
            }],
            temperature: 0.7,
            max_tokens: Some(1000),
        };

        let response = provider.complete(&request).await?;

        Ok(DraftSuggestion {
            content: response.text,
            confidence: 0.8, // Could be refined based on model confidence
        })
    }

    /// Performs semantic search across emails.
    ///
    /// Uses embeddings to find semantically similar emails regardless
    /// of exact keyword matches.
    ///
    /// # Arguments
    ///
    /// * `query` - Natural language search query
    /// * `account_ids` - Accounts to search within
    ///
    /// # Returns
    ///
    /// A list of search results sorted by relevance.
    pub async fn semantic_search(
        &self,
        query: &str,
        _account_ids: &[AccountId],
    ) -> Result<Vec<SearchResult>> {
        let settings = self.settings.read().await;
        if !settings.enabled || !settings.search_settings.enabled {
            anyhow::bail!("Semantic search is disabled");
        }

        let engine_guard = self.embedding_engine.read().await;
        let engine = engine_guard
            .as_ref()
            .ok_or_else(|| anyhow::anyhow!("No embedding engine configured"))?;

        // Generate query embedding
        let query_embedding = engine.embed(query).await?;

        // Search for similar emails
        let results = engine
            .search(&query_embedding, settings.search_settings.max_results)
            .await?;

        // Filter by minimum relevance and convert to SearchResult
        let search_results: Vec<SearchResult> = results
            .into_iter()
            .filter(|(_, score)| *score >= settings.search_settings.min_relevance)
            .map(|(email_id, relevance)| SearchResult {
                email_id,
                thread_id: crate::domain::ThreadId::from(""), // Would be filled from storage
                subject: None,
                snippet: String::new(),
                relevance,
                highlights: vec![],
            })
            .collect();

        Ok(search_results)
    }

    /// Categorizes an email into one or more categories.
    ///
    /// Uses AI to determine the likely categories for an email based
    /// on its content, sender, and metadata.
    ///
    /// # Arguments
    ///
    /// * `email` - The email to categorize
    ///
    /// # Returns
    ///
    /// A list of categories, ordered by confidence.
    pub async fn categorize_email(&self, email: &Email) -> Result<Vec<Category>> {
        let settings = self.settings.read().await;
        if !settings.enabled {
            anyhow::bail!("AI features are disabled");
        }

        let provider = self.get_provider(None).await?;

        let email_content = format!(
            "From: {}\nSubject: {}\n\n{}",
            email.from.display(),
            email.subject.as_deref().unwrap_or("(no subject)"),
            email.snippet
        );

        let categories_list = Category::all()
            .iter()
            .map(|c| c.display_name())
            .collect::<Vec<_>>()
            .join(", ");

        let request = CompletionRequest {
            system_prompt: Some(format!(
                "Categorize the following email into one or more categories: {}. \
                 Return only the category names, comma-separated, most relevant first.",
                categories_list
            )),
            messages: vec![ChatMessage {
                role: ChatRole::User,
                content: email_content,
            }],
            temperature: 0.2,
            max_tokens: Some(100),
        };

        let response = provider.complete(&request).await?;

        // Parse categories from response
        let categories: Vec<Category> = response
            .text
            .split(',')
            .filter_map(|s| {
                let s = s.trim().to_lowercase();
                match s.as_str() {
                    "primary" => Some(Category::Primary),
                    "social" => Some(Category::Social),
                    "promotions" => Some(Category::Promotions),
                    "updates" => Some(Category::Updates),
                    "forums" => Some(Category::Forums),
                    "finance" => Some(Category::Finance),
                    "travel" => Some(Category::Travel),
                    "shopping" => Some(Category::Shopping),
                    "newsletters" => Some(Category::Newsletters),
                    "work" => Some(Category::Work),
                    "personal" => Some(Category::Personal),
                    _ => None,
                }
            })
            .collect();

        Ok(categories)
    }

    /// Analyzes a sender to determine their likely type.
    ///
    /// Uses AI and historical data to categorize senders for the screener.
    ///
    /// # Arguments
    ///
    /// * `sender` - The sender email address to analyze
    ///
    /// # Returns
    ///
    /// A sender analysis with type, reasoning, and suggested action.
    pub async fn analyze_sender(&self, sender: &str) -> Result<SenderAnalysis> {
        let settings = self.settings.read().await;
        if !settings.enabled {
            anyhow::bail!("AI features are disabled");
        }

        let provider = self.get_provider(None).await?;

        let request = CompletionRequest {
            system_prompt: Some(
                "Analyze the sender email address and determine their likely type. \
                 Categories: known_contact, newsletter, marketing, recruiter, support, unknown. \
                 Suggest whether to approve, reject, or review. \
                 Respond in format: TYPE|REASONING|ACTION"
                    .to_string(),
            ),
            messages: vec![ChatMessage {
                role: ChatRole::User,
                content: format!("Analyze sender: {}", sender),
            }],
            temperature: 0.2,
            max_tokens: Some(200),
        };

        let response = provider.complete(&request).await?;

        // Parse the response
        let parts: Vec<&str> = response.text.split('|').collect();

        let likely_type = if parts.len() > 0 {
            match parts[0].trim().to_lowercase().as_str() {
                "known_contact" => crate::domain::SenderType::KnownContact,
                "newsletter" => crate::domain::SenderType::Newsletter,
                "marketing" => crate::domain::SenderType::Marketing,
                "recruiter" => crate::domain::SenderType::Recruiter,
                "support" => crate::domain::SenderType::Support,
                _ => crate::domain::SenderType::Unknown,
            }
        } else {
            crate::domain::SenderType::Unknown
        };

        let reasoning = if parts.len() > 1 {
            parts[1].trim().to_string()
        } else {
            "Unable to analyze sender".to_string()
        };

        let suggested_action = if parts.len() > 2 {
            match parts[2].trim().to_lowercase().as_str() {
                "approve" => crate::domain::ScreenerAction::Approve,
                "reject" => crate::domain::ScreenerAction::Reject,
                _ => crate::domain::ScreenerAction::Review,
            }
        } else {
            crate::domain::ScreenerAction::Review
        };

        Ok(SenderAnalysis {
            likely_type,
            reasoning,
            suggested_action,
        })
    }

    /// Formats a thread for summarization.
    fn format_thread_for_summary(&self, thread: &Thread) -> String {
        let mut content = format!(
            "Subject: {}\nParticipants: {}\n\n",
            thread.subject.as_deref().unwrap_or("(no subject)"),
            thread
                .participants
                .iter()
                .map(|p| p.display())
                .collect::<Vec<_>>()
                .join(", ")
        );

        for email in &thread.messages {
            content.push_str(&format!(
                "---\nFrom: {}\nDate: {}\n\n{}\n",
                email.from.display(),
                email.date.format("%Y-%m-%d %H:%M"),
                email.body_text.as_deref().unwrap_or(&email.snippet)
            ));
        }

        content
    }

    /// Formats a thread for reply generation.
    fn format_thread_for_reply(&self, thread: &Thread) -> String {
        let mut content = format!(
            "Thread subject: {}\n\n",
            thread.subject.as_deref().unwrap_or("(no subject)")
        );

        // Include recent messages for context (last 5)
        for email in thread.messages.iter().rev().take(5).rev() {
            content.push_str(&format!(
                "---\nFrom: {}\nDate: {}\n\n{}\n",
                email.from.display(),
                email.date.format("%Y-%m-%d %H:%M"),
                email.body_text.as_deref().unwrap_or(&email.snippet)
            ));
        }

        content
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn summary_parse_basic() {
        let text =
            "This is a summary.\n\nKey points:\n- Point 1\n- Point 2\n\nAction items:\n- Task 1";
        let summary = Summary::parse(text);

        assert_eq!(summary.text, "This is a summary.");
        assert_eq!(summary.key_points, vec!["Point 1", "Point 2"]);
        assert_eq!(summary.action_items, vec!["Task 1"]);
    }

    #[test]
    fn summary_parse_no_sections() {
        let text = "Just a simple summary without sections.";
        let summary = Summary::parse(text);

        assert_eq!(summary.text, "Just a simple summary without sections.");
        assert!(summary.key_points.is_empty());
        assert!(summary.action_items.is_empty());
    }

    #[test]
    fn category_display_names() {
        assert_eq!(Category::Primary.display_name(), "Primary");
        assert_eq!(Category::Promotions.display_name(), "Promotions");
        assert_eq!(Category::Work.display_name(), "Work");
    }

    #[test]
    fn category_all_variants() {
        let all = Category::all();
        assert_eq!(all.len(), 11);
        assert!(all.contains(&Category::Primary));
        assert!(all.contains(&Category::Personal));
    }

    #[test]
    fn category_serialization() {
        let category = Category::Finance;
        let json = serde_json::to_string(&category).unwrap();
        assert_eq!(json, "\"finance\"");

        let deserialized: Category = serde_json::from_str(&json).unwrap();
        assert_eq!(deserialized, Category::Finance);
    }

    #[test]
    fn ai_settings_default() {
        let settings = AiSettings::default();
        assert!(settings.enabled);
        assert_eq!(settings.default_provider, "anthropic");
        assert!(settings.summary_settings.enabled);
    }

    #[test]
    fn tone_serialization() {
        let tone = Tone::Formal;
        let json = serde_json::to_string(&tone).unwrap();
        assert_eq!(json, "\"formal\"");

        let custom = Tone::Custom("Be sarcastic".to_string());
        let custom_json = serde_json::to_string(&custom).unwrap();
        assert!(custom_json.contains("Be sarcastic"));
    }

    #[test]
    fn search_result_serialization() {
        let result = SearchResult {
            email_id: EmailId::from("email-1"),
            thread_id: crate::domain::ThreadId::from("thread-1"),
            subject: Some("Test".to_string()),
            snippet: "Preview...".to_string(),
            relevance: 0.95,
            highlights: vec!["matching text".to_string()],
        };

        let json = serde_json::to_string(&result).unwrap();
        let deserialized: SearchResult = serde_json::from_str(&json).unwrap();

        assert_eq!(deserialized.relevance, 0.95);
        assert_eq!(deserialized.highlights.len(), 1);
    }

    #[test]
    fn draft_suggestion_serialization() {
        let draft = DraftSuggestion {
            content: "Hello, thank you for your email.".to_string(),
            confidence: 0.85,
        };

        let json = serde_json::to_string(&draft).unwrap();
        let deserialized: DraftSuggestion = serde_json::from_str(&json).unwrap();

        assert_eq!(deserialized.confidence, 0.85);
    }
}
